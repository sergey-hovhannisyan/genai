{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import json\n",
    "from typing import Dict, Tuple, List\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/epirecipes/full_format_recipes.json') as json_data:\n",
    "    text = json.load(json_data)\n",
    "\n",
    "text = [\n",
    "    'Recipe for ' + x['title'] + ' | ' + ''.join(x['directions'])\n",
    "    for x in text\n",
    "    if 'title' in x\n",
    "    and x['title'] is not None\n",
    "    and 'directions' in x\n",
    "    and x['directions'] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "def pad_punctuation(s:str):\n",
    "    s = re.sub(f\"([{string.punctuation}])\", r' \\1 ', s)\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    return s.lower()\n",
    "\n",
    "text = [pad_punctuation(x) for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "VOCAB_SIZE = 10\n",
    "word_count = Counter(word for recipe in text for word in recipe.split())\n",
    "special_tokens = ['<pad>', '<unk>', '<eos>']\n",
    "vocab = special_tokens + [word for word, _ in word_count.most_common(VOCAB_SIZE)]\n",
    "VOCAB_SIZE += len(special_tokens)\n",
    "word2idx = {word:index for index, word in enumerate(vocab)}\n",
    "tokenizer = lambda x: [word2idx.get(word, word2idx['<unk>']) for word in x.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Epirecipes(Dataset):\n",
    "    def __init__(self, data:List[List[int]], seq_len:int):\n",
    "        self.data = [[word2idx['<pad>']] * seq_len + \n",
    "                     recipe + [word2idx['<eos>']] for recipe in data]\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __getitem__(self, index) -> torch.Tensor:\n",
    "        rand_idx = np.random.randint(0, len(self.data[index]) - self.seq_len)\n",
    "        X =  torch.tensor(self.data[index][rand_idx:rand_idx+self.seq_len])\n",
    "        y = torch.tensor(self.data[index][rand_idx+self.seq_len])\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "SEQ_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "recipes = Epirecipes([tokenizer(x) for x in text], SEQ_LEN)\n",
    "dataloader = DataLoader(recipes, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Cell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size \n",
    "        \n",
    "        self.W_input = nn.Linear(input_size, 4 * hidden_size)\n",
    "        torch.nn.init.xavier_normal_(self.W_input.weight)\n",
    "        torch.nn.init.constant_(self.W_input.bias, 0.0)\n",
    "        \n",
    "    def forward(self, x, h_state, c_state):\n",
    "        combined = torch.cat((x, h_state), dim=-1)\n",
    "        gate_inputs = self.W_input(combined)\n",
    "        f_gate, i_gate, c_gate, o_gate = torch.chunk(gate_inputs, chunks=4, dim=-1)\n",
    "        f_gate = torch.sigmoid(f_gate)\n",
    "        i_gate = torch.sigmoid(i_gate)\n",
    "        c_gate = torch.tanh(c_gate)\n",
    "        o_gate = torch.sigmoid(o_gate)\n",
    "        \n",
    "        new_c_state = i_gate * c_gate + f_gate * c_state\n",
    "        new_h_state = o_gate * torch.tanh(new_c_state)\n",
    "        \n",
    "        return new_h_state, new_c_state\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embd_size:int, hidden_size:Tuple, num_layers:int, vocab_size:int, batch_size:int):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embd_size = embd_size \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embd_size)\n",
    "        self.input_size = [sum(pair) for pair in zip([embd_size] + list(hidden_size[:-1]), hidden_size)]\n",
    "        self.layers = nn.ModuleList([Cell(self.input_size[i], self.hidden_size[i]) for i in range(num_layers)])\n",
    "        \n",
    "        # -----------------> experimental\n",
    "        self.output = nn.Sequential(\n",
    "                nn.Linear(hidden_size[-1], vocab_size),\n",
    "                nn.Softmax(dim=-1))\n",
    "                \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        batch_size = X.shape[0]\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        G = list(X)\n",
    "        device = X.device\n",
    "        for j in range(self.num_layers):\n",
    "            h_state = torch.zeros((batch_size, self.hidden_size[j])).to(device)\n",
    "            c_state = torch.zeros_like(h_state)\n",
    "            for i in range(len(G)):\n",
    "                h_state, c_state = self.layers[j](G[i], h_state, c_state)\n",
    "                G[i] = h_state\n",
    "        # -----------> experimental \n",
    "        out = self.output(h_state)\n",
    "        return out\n",
    "\n",
    "    def generate(self, X:str, seq_len:int=256, temperature: float=1.0) -> str:\n",
    "        X = self.to_input(X, seq_len).tolist()\n",
    "        self.eval()\n",
    "        with torch.inference_mode():\n",
    "            while X[-1] is not word2idx['<eos>']:\n",
    "                out = self(torch.tensor(X[-seq_len:]).unsqueeze(dim=0).to(device))\n",
    "                out = out.pow(1 / temperature)\n",
    "                out /= out.sum(dim=1, keepdim=True)\n",
    "                out = torch.multinomial(out, 1).item()\n",
    "                X.append(out)\n",
    "        return self.to_text(X)\n",
    "    \n",
    "    def to_input(self, X:str, seq_len:int=256) -> torch.Tensor:\n",
    "        X = tokenizer(X)\n",
    "        X = X[-seq_len:] if len(X) > seq_len else X\n",
    "        padding_size = seq_len - len(X)\n",
    "        X = [word2idx['<pad>']] * (padding_size) + X\n",
    "        return torch.tensor(X)\n",
    "        \n",
    "    def to_text(self, X:torch.Tensor) -> str:\n",
    "        X = \" \".join([vocab[token] for token in X if vocab[token] not in special_tokens])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model: nn.Module, dataloader: DataLoader,\n",
    "            optimizer: Optimizer, loss_fn: nn.Module, \n",
    "            device: torch.device, EPOCHS: int, \n",
    "            test: torch.Tensor, metric: callable=None) -> None:\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    report = {\"loss\": [], metric.__name__: []}\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch: ---------------> {epoch}/{EPOCHS}\")\n",
    "        itr_loss = .0\n",
    "        itr_metric = .0\n",
    "        \n",
    "        for X, y in tqdm(dataloader):\n",
    "            X, y = X.to(device), y.to(device)        \n",
    "            logit = model(X)\n",
    "            loss = loss_fn(logit, y)\n",
    "            \n",
    "            itr_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        itr_loss /= len(dataloader)\n",
    "        itr_metric = metric(model, test)\n",
    "        \n",
    "        print(f\"Loss: ---------------> {itr_loss:.4f}\")\n",
    "        print(f\"{metric.__name__} ---------> {itr_metric:.4f}\")\n",
    "        \n",
    "        report['loss'].append(itr_loss)\n",
    "        report[metric.__name__].append(itr_metric)\n",
    "\n",
    "                \n",
    "SEQ_LEN = 16\n",
    "HIDDEN_SIZE = (10, 20, 30)\n",
    "EMBD_SIZE = 10\n",
    "NUM_LAYERS = 3\n",
    "LEARNING_RATE = .0001\n",
    "BETAS = (0.9, 0.99)\n",
    "EPOCHS = 5\n",
    "\n",
    "def perplexity(model: nn.Module, test: torch.Tensor, context_len: int) -> float:\n",
    "    ppl = .0\n",
    "    for recipe in test: \n",
    "        padded = word2idx['<pad>'] * (context_len - 1) + recipe\n",
    "        \n",
    "\n",
    "n_test_samples = int(len(text) * 0.1)\n",
    "test_text = np.random.choice(text, size=n_test_samples, replace=False)\n",
    "test = [tokenizer(recipe) for recipe in test_text]\n",
    "\n",
    "\n",
    "model = LSTM(EMBD_SIZE, HIDDEN_SIZE, NUM_LAYERS, VOCAB_SIZE, BATCH_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=BETAS)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "trainer(model, dataloader, optimizer, loss_fn, device, EPOCHS, test, perplexity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
